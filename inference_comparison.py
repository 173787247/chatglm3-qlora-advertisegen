#!/usr/bin/env python3
"""
ChatGLM3-6B QLoRA å¾®è°ƒå‰åæ•ˆæœå¯¹æ¯”è„šæœ¬

æœ¬è„šæœ¬ç”¨äºå¯¹æ¯”å¾®è°ƒå‰åçš„ ChatGLM3-6B æ¨¡å‹åœ¨ AdvertiseGen æ•°æ®é›†ä¸Šçš„ç”Ÿæˆæ•ˆæœã€‚
å¯ä»¥åœ¨ Cursor ä¸­ç›´æ¥è¿è¡Œï¼Œä¹Ÿå¯ä»¥åœ¨ Docker å®¹å™¨ä¸­è¿è¡Œã€‚
"""

import torch
from transformers import AutoModel, AutoTokenizer
from peft import PeftModel, PeftConfig
import pandas as pd
from pathlib import Path


def load_models():
    """åŠ è½½åŸºç¡€æ¨¡å‹å’Œå¾®è°ƒåçš„æ¨¡å‹"""
    model_name_or_path = 'THUDM/chatglm3-6b'
    
    print("=" * 80)
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    print("=" * 80)
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    print("\n1. åŠ è½½åŸºç¡€æ¨¡å‹...")
    base_model = AutoModel.from_pretrained(
        model_name_or_path,
        device_map='auto',
        trust_remote_code=True
    )
    base_model.requires_grad_(False)
    base_model.eval()
    print("âœ… åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # åŠ è½½ Tokenizer
    print("\n2. åŠ è½½ Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        model_name_or_path,
        trust_remote_code=True
    )
    print("âœ… Tokenizer åŠ è½½å®Œæˆ")
    
    # åŠ è½½å¾®è°ƒåçš„æ¨¡å‹
    print("\n3. åŠ è½½å¾®è°ƒåçš„æ¨¡å‹...")
    peft_model_path = "outputs/chatglm3-qlora"
    config = PeftConfig.from_pretrained(peft_model_path)
    fine_tuned_model = PeftModel.from_pretrained(base_model, peft_model_path)
    fine_tuned_model.eval()
    print("âœ… å¾®è°ƒåçš„æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"è®­ç»ƒé…ç½®: LoRA r={config.r}, alpha={config.lora_alpha}, dropout={config.lora_dropout}")
    
    return base_model, fine_tuned_model, tokenizer


def compare_responses(query, base_model, fine_tuned_model, tokenizer):
    """å¯¹æ¯”åŸºç¡€æ¨¡å‹å’Œå¾®è°ƒåæ¨¡å‹çš„ç”Ÿæˆç»“æœ"""
    device = next(base_model.parameters()).device
    
    # åŸºç¡€æ¨¡å‹ç”Ÿæˆï¼ˆä½¿ç”¨ generate æ–¹æ³•ï¼Œæ›´å…¼å®¹ï¼‰
    try:
        # å°è¯•ä½¿ç”¨ chat æ–¹æ³•
        base_response, _ = base_model.chat(tokenizer, query=query, history=[])
    except AttributeError:
        # å¦‚æœ chat æ–¹æ³•å¤±è´¥ï¼Œä½¿ç”¨ generate æ–¹æ³•
        inputs = tokenizer(query, return_tensors="pt").to(device)
        with torch.no_grad():
            outputs = base_model.generate(
                **inputs,
                max_new_tokens=512,
                do_sample=True,
                temperature=0.8,
                top_p=0.8,
                pad_token_id=tokenizer.pad_token_id,
            )
        base_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        # ç§»é™¤è¾“å…¥éƒ¨åˆ†ï¼Œåªä¿ç•™ç”Ÿæˆçš„éƒ¨åˆ†
        base_response = base_response.replace(query, "").strip()
    
    # å¾®è°ƒåæ¨¡å‹ç”Ÿæˆ
    try:
        fine_tuned_response, _ = fine_tuned_model.chat(tokenizer, query=query, history=[])
    except AttributeError:
        # å¦‚æœ chat æ–¹æ³•å¤±è´¥ï¼Œä½¿ç”¨ generate æ–¹æ³•
        inputs = tokenizer(query, return_tensors="pt").to(device)
        with torch.no_grad():
            outputs = fine_tuned_model.generate(
                **inputs,
                max_new_tokens=512,
                do_sample=True,
                temperature=0.8,
                top_p=0.8,
                pad_token_id=tokenizer.pad_token_id,
            )
        fine_tuned_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        # ç§»é™¤è¾“å…¥éƒ¨åˆ†ï¼Œåªä¿ç•™ç”Ÿæˆçš„éƒ¨åˆ†
        fine_tuned_response = fine_tuned_response.replace(query, "").strip()
    
    return base_response, fine_tuned_response


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("ChatGLM3-6B QLoRA å¾®è°ƒå‰åæ•ˆæœå¯¹æ¯”")
    print("=" * 80)
    
    # åŠ è½½æ¨¡å‹
    base_model, fine_tuned_model, tokenizer = load_models()
    
    # æµ‹è¯•æç¤ºè¯ï¼ˆæ¥è‡ª AdvertiseGen æ•°æ®é›†ï¼‰
    test_prompts = [
        "å†™ä¸€æ®µ 30 å­—çš„å¹¿å‘Šæ–‡æ¡ˆï¼Œæ¨å¹¿æ™ºèƒ½ç†è´¢æœåŠ¡",
        "ä¸ºä¸€å®¶ç²¾å“é…’åº—æ’°å†™ä¸€æ¡ä¿ƒé”€æ¨é€é€šçŸ¥",
        "ä»¥äº²åˆ‡çš„å£å»ï¼Œå†™ä¸€ä¸ªä½“è‚²ç”¨å“å“ç‰Œçš„å¼•å¯¼è´­ä¹°å¯¹è¯"
    ]
    
    print("\n" + "=" * 80)
    print("å¾®è°ƒå‰åæ•ˆæœå¯¹æ¯”")
    print("=" * 80)
    
    # å®æ—¶å¯¹æ¯”æµ‹è¯•
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\n{'='*80}")
        print(f"æµ‹è¯•æ ·æœ¬ {i}")
        print(f"{'='*80}")
        print(f"\nğŸ“ Promptï¼ˆæç¤ºè¯ï¼‰:")
        print(f"   {prompt}")
        
        # è·å–å¯¹æ¯”ç»“æœ
        base_response, fine_tuned_response = compare_responses(
            prompt, base_model, fine_tuned_model, tokenizer
        )
        
        print(f"\nğŸ”µ å¾®è°ƒå‰è¾“å‡º:")
        print(f"   {base_response}")
        
        print(f"\nğŸŸ¢ å¾®è°ƒåè¾“å‡º:")
        # æ¸…ç†è¾“å‡ºæ–‡æœ¬ï¼ˆç§»é™¤ç‰¹æ®Šæ ‡è®°ï¼‰
        cleaned_response = fine_tuned_response.replace('[gMASK]', '').replace('sop', '').strip()
        print(f"   {cleaned_response}")
        print()
    
    # æ˜¾ç¤ºè®­ç»ƒæ—¶ä¿å­˜çš„å¯¹æ¯”ç»“æœ
    print("\n" + "=" * 80)
    print("è®­ç»ƒæ—¶ä¿å­˜çš„å¾®è°ƒå‰åå¯¹æ¯”ç»“æœ")
    print("=" * 80)
    
    comparison_file = Path("outputs/chatglm3-qlora/prompt_comparison.csv")
    if comparison_file.exists():
        comparison_df = pd.read_csv(comparison_file)
        print(f"\nâœ… è¯»å–äº† {len(comparison_df)} ä¸ªæµ‹è¯•æ ·æœ¬\n")
        
        for idx, row in comparison_df.iterrows():
            print(f"{'='*80}")
            print(f"æ ·æœ¬ {idx + 1}")
            print(f"{'='*80}")
            print(f"\nğŸ“ Promptï¼ˆæç¤ºè¯ï¼‰:")
            print(f"   {row['prompt']}")
            
            print(f"\nğŸ”µ å¾®è°ƒå‰è¾“å‡º:")
            if row['pretraining_response'] == '[baseline skipped]':
                print("   [åŸºçº¿æµ‹è¯•å·²è·³è¿‡]")
            else:
                pre_text = str(row['pretraining_response']).replace('[gMASK]', '').replace('sop', '').strip()
                print(f"   {pre_text[:300]}..." if len(pre_text) > 300 else f"   {pre_text}")
            
            print(f"\nğŸŸ¢ å¾®è°ƒåè¾“å‡º:")
            post_text = str(row['posttraining_response']).replace('[gMASK]', '').replace('sop', '').strip()
            # ç§»é™¤é‡å¤å†…å®¹
            if post_text.count('"') > 4:
                lines = post_text.split('\n')
                unique_lines = []
                seen = set()
                for line in lines:
                    line_clean = line.strip().strip('"').strip()
                    if line_clean and line_clean not in seen:
                        unique_lines.append(line_clean)
                        seen.add(line_clean)
                post_text = '\n'.join(unique_lines)
            
            print(f"   {post_text[:500]}..." if len(post_text) > 500 else f"   {post_text}")
            print()
    else:
        print(f"\nâš ï¸  æœªæ‰¾åˆ°å¯¹æ¯”æ–‡ä»¶: {comparison_file}")
    
    print("\n" + "=" * 80)
    print("å¯¹æ¯”å®Œæˆï¼")
    print("=" * 80)


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

